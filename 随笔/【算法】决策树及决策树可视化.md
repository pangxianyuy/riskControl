
### 在决策树之前需要进行特征选项

[特征选择的链接](https://www.zhihu.com/question/29316149)

[随机森林和GBDT](https://www.cnblogs.com/zichun-zeng/p/6296820.html)


[决策树+bagging方法=随机森林RF](https://easyai.tech/ai-definition/random-forest/)


[分类树和回归树的区分](https://blog.csdn.net/puqutogether/article/details/44593647)





```python
#树结构可视化
import pydotplus
dot = tree.export_graphviz(clf6,out_file=None,feature_names=list(jf69.columns[:-9]))
graph = pydotplus.graph_from_dot_data(dot)
graph.write_pdf('./tree.pdf')
```


### GBDT与随机森林的对比
- 相同点
  
    都是由多棵树组成。最终的结果都是由多棵树一起决定。
- 不同点
    
    组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成。
    组成随机森林的树可以并行生成；而GBDT只能是串行生成。
    对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。

### bagging与boosting的对比
  - -不同点
    - 二者的主要区别是 取样方式不同 。bagging采用 均匀取样 ，而Boosting根据错误率来取样，因此boosting的分类精度要优于Bagging。

    - bagging的训练集的选择是随机的，各轮训练集之间相互独立，而boostlng的各轮训练集的选择与前面各轮的学习结果有关；bagging的各个预测函数没有权重，而boosting是有权重的；

    - bagging的各个预测函数可以并行生成，而boosting的各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法。bagging可通过并行训练节省大量时间开销。

    - bagging和boosting都可以有效地提高分类的准确性。在大多数数据集中，boosting的准确性比bagging高。在有些数据集中，boosting会引起退化---过拟合（Overfit）。
    - Boosting思想的一种改进型AdaBoost方法在邮件过滤、文本分类方面都有很好的性能。

    - Gradient boosting（又叫Mart, Treenet)：Boosting是一种思想，Gradient Boosting是一种实现Boosting的方法，它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数(loss function)描述的是模型的不靠谱程度，损失函数越大，则说明模型越容易出错。如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度（Gradient)的方向上下降。



